## Introduction

Parallel system - один синхронный input,
один синхронный output, параллельность только внутри

Concurrent system - много асинхронных input-ов,
надо синхронизировать их ручками

### PRAM - parallel random access machine.

Набор синхронно выполняющихся инструкций, каждое ядрышко по одному такту.
Инструкции - некоторые простые операции - чтение/запись/сложение и тд.

**Work** - суммарное количество выполненных операций (включая пустые)

**Time** - количество итераций

**Разрешение конфликтов в PRAM**
- EREW - exclusive read, exclusive write
- CREW - concurrent read, exclusive write
- CRCW - concurrent read, concurrent write
    - arbitrary - какое-то из значений будет записано
    - common - все должны записать одно и то же
    - priority - процесс с наибольшим приоритетом выигрывает

Есть теорема, заявляющая, что можно
эмулировать CRCW на EREW за (число итераций) * log(n).

```
Input:
  n - number of processes = 2^k
  id: Process number, 1..n
  A - input array, size n, indices 1..n
  B - output array, same as A


a <- A[id]
B[id] <- a
for h in 1..log(n)
    if id < n / 2^h:
        x <- B[2 * id + 1]
        y <- B[2 * id]
        z <- x + y
        B[id] <- z
    else:
        skip(4)
if id == 1:
    ans <- A[1]


Work: O(n * log(n))
Time: O(log(n))
```


### Parallel for (work-depth model)

Есть parallel for, который выполняется на скольки-то процессах.
Он же - точка синхронизации.
Идея - запрятать число процессов куда-то внутрь parallel for-а.
Тело pfor-а выполняется процессорами по одной инструкции.

```
pfor i in 1..n:
    a <- A[id]
    B[id] <- a
for h in i..log(n):
    pfor i in 1..n/2:
        x <- B[2 * id + 1]
        y <- B[2 * id]
        z <- x + y
        B[id] <- z


Work: O(n)        // процессы не простаивают
Depth: O(log(n))  // если число процессов бесконечно
```

### Fork-jon model

Все исполнение программы можно представить в виде
ориентированного ациклического графа (DAG-а).
Разветвления и слияния в таком графе - это fork() или join(),
они же - точки синхронизации.
Каждую вершину такого графа мы считаем за O(1),
однако для удобства можно обозначить бамбук в графе за одну не-O(1)-вершину.
У такого исполнения есть work и span.

**Work** - количество вершин в DAG-е

**Span** - время выполнения при бесконечном числе процессов,
оно же - самый длинный путь в DAG-е

pfor на fork-join модели требует O(n) work и O(log(n)) span.

Из приятного:

```
pfor i = 1..n:
    pfor j = 1..m:
        ++x[i][j]

Work: O(n * m)
Span: O(log(n) + log(m))
```

Из не очень приятного - Scheduling

**Теорема Брента:**

Возьмем level-by-level scheduler.
Тогда время T его работы будет `T <= W / P + S * (P - 1) / P`

Док-во: Назовем W[i] кол-во вершин на i-м шаге. Тогда
```
T = Sum(i = 1..S) { ceil(W[i]/P) } <= 
    <= Sum(..) { W[i]/P + 1 - 1/P } =
    = W/P + S * (P - 1)/P
```

**Ещё теорема: любой Greedy Scheduler подходит под то же условие**

Work token - платим 1, если сделали что-то полезное

Idle token - платим 1, если отдыхаем

Work token-ов, очевидно, W. Сколько idle токенов?

Утверждение - каждый раз, когда мы платим dile token,
оставшийся span становится на 1 меньше.
Кажется, можно доказать по индукции - для одного запуска это правда,
для остальных тогда +- очевидно.

Тогда idle токенов может быть не более, чем S * (P - 1)

Тогда суммарно токенов - W + S * (P - 1)

Суммарное время `T <= W / P + S * (P - 1) / P

Когда делаем алгоритм:
1. Ищем work-optimal алгоритм
2. После этого уменьшаем span

В рамках курса пытаемся сделать Work - полином, Span - полилог,
но ирл, поскольку процессов мало, достаточно условного sqrt(n) Span.



## Sequence algorithms

Классические алгоритмы на Sequence<T>:
- `map<R>(mapper: (T) -> R): Sequence<R>`
- `reduce<R>(neutral: R, associative: (R, T) -> R): R`
- `scan<R>(neutral: R, associative: (R, T) -> R): Sequence<R>`
- `filter(indicator: (T) -> Bool): Sequence<T>`

Разумеется, можно написать тупую реализацию без BLOCK,
но жить с таким будет невозможно.

Задачка подбора размера блока - granularity control problem -
в общем случае весьма нетривиальна.


```
fun parallelFor(
    block: Int,
    l: Int,
    r: Int,
    action: (index: Int) -> Unit,
) {
    if (r - l <= blockSize) {
        for (i in l until r) {
            action(i)
        }
        return
    }
    join(
        fork { parallelFor(block, l, m, action) },
        fork { parallelFor(block, m, r, action) },
    )
}

Work: O(n)
Span: O(log(n / BLOCK) + BLOCK)
```

```
fun List<T>.parallelMap(
    block: Int,
    l: Int,
    r: Int,
    target: MutableList<R?> = MutableList(size) { null },
    mapper: (T) -> R,
): List<R> {
    if (r - l <= block) {
        for (i in l until r) {
            target[i] = mapper(this[i])
            mapper(i)
        }
        return
    }
    joinAll(
        launch { parallelMap(block, l, m, target, mapper) },
        launch { parallelMap(block, m, r, target, mapper) },
    )
    return target as List<R>
}
```

```
fun List<T>.parallelReduce(
    block: Int,
    l: Int,
    r: Int,
    associative: (T, T) -> T,
): List<T> {
    if (r - l <= block) {
        var result = this[l]
        for (i in l.inc() until r) {
            result = associative(result, this[i])
        }
        return result
    }
    val l = async { parallelReduce(block, l, m, associative) }
    val r = async { parallelReduce(block, m, r, associative) }
    return mapper(l, r)
}
```

```
fun List<T>.parallelScan(
    block: Int,
    l: Int,
    r: Int,
    target: MutableList<T?> = MutableList(size) { null },
    associative: (T, T) -> T,
): List<R> {
    if (r - l <= block) {
        target[l] = this[l]
        for (i in l.inc() until r) {
            target[i] = associative(target[i - 1], this[i])
        }
        return target
    }

    val sums = (0 until (n + block - 1) / block).map { blockIndex ->
        async {
            val l = blockIndex * block
            val r = minOf(size, l + block)
            reduce(block, l, r, associative)
        }
    }.awaitAll()

    (0 until (n + block - 1) / block).map { blockIndex ->
        launch {
            val l = blockIndex * block
            val r = minOf(size, l + block)
            target[l] = associative(sums[blockIndex], this[l])
            for (i in l.inc() until r) {
                target[i] = associative(target[i - 1], this[i])
            }
        }
    }.joinAll()

    return target
}
```

```
fun List<T>.parallelFilter(
    block: Int,
    indicator: (T) -> Bool,
): List<T> {
    val flags = MutableList<Int> { 0 }
    val counts = (0 until (n + block - 1) / block).map { blockIndex ->
        async {
            var count = 0
            val l = blockIndex * block
            val r = minOf(size, l + block)
            
            for (i in l until r) {
                if (indicator(this[i])) {
                    flags = 1
                    ++count
                }
            }
            return@async count 
        }
    }.awaitAll()
    val sums = counts.parallelScan(block, 0, counts.size, Int::plus)

    val target = MutableList<T?>(sums.last() + counts.last()) { null }

    (0 until (n + block - 1) / block).map { blockIndex ->
        launch {
            val l = blockIndex * block
            val r = minOf(size, l + block)
            var position = sums[blockIndex]

            for (i in l until r) {
                if (flags[i]) {
                    target[position++] = this[i]
                }
            }
        }
    }.joinAll()

    return target
}
```



## Divide-and-Conquer

Будем рекурсивно делить задачу на подзадачи
и затем собирать итоговый результат из результатов подзадач.

### Пример: mergesort

```
fun List<T>.mergeSorted(
    block: Int,
    l: Int,
    r: Int,
): List<T> {
    if (r - l < block) {
        return this.view(l, r).sorted()
    }
    val m = (l + r) / 2
    val left = async { mergeSorted(block, l, m) }
    val right = async { mergeSorted(block, m, r) }
    return merge(left.await(), right.await())
}
```

Допустим, span merge = O(M(n)). Тогда

```
Work = O(n * log(n)),
Span = O(log(n) * (M(n) + M(n / 2) + M(n / 4) + ...)) =
     = O(log(n) * M(n))    // если M - полилог
```

### Параллельный merge:

Тупой вариант:

За Work = O(n * log(n)) и Span = O(log(n)) найдем каждому элементу
каждого массива его место в результирующем массиве через бин поиск.

Вариант поумнее:

Побьем каждый массив на блоки размера log(n).
Для каждого старта блока из первого массива
найдем его место во втором бин поиском,
аналогично - для слишком больших блоков из 2-го массива.

Тогда merge каждого куска у нас за log(n) work, а total merge-а такой:
```
Span = O(log(n))
Work = O(n / log(n) * log(n)) = O(n)
```



## Accelerating cascades

### K-я порядковая статистика

Тупое решение - отсортировать. Work = O(n * log(n)), Span = O(log^2(n))

k = n / log(n)
O(k * log(k)) = n / log(n) * log(n / log(n)) =

Сведение задачи к меньшей:

- Побьем на блоки по log(n)
- Возьмем медиану в каждом из них за O(log(n))
- Найдем медиану медиан тупым решением за work = O(n), span = O(log^2(n))
- С помощью итоговой медианы поделим массив и выберем нужную половину

Сведем задачу к меньшей log(log(n)) раз, чтобы получить размер n / log(n)

Тогда work = O(n), span = O(log^2(n) * log(log(n)))

### max в common CRCW

Заведем табличку t размера n^2 заполненную true
```
pfor i in 1..n:
    pfor j in 1..n:
        if (arr[i] < arr[j]):
            is_max[i] = false
pfor i in 1..n:
    if (is_max[i]):
        ans = i

Work = O(n^2)
Depth = O(1)
``` 

Work ужасный, попробуем быстрее

```
n = 2 ^ (2 ^ k)
Заведем дерево, где сверху вниз у каждой вершины детей будет:
2^(2 ^ (k - 1))
2^(2 ^ (k - 2))
...
2^(2 ^ 0) = 2
Очевидно, что таких уровней будет log(log(n))

На каждом уровне для каждой вершины запустим предыдущий алгос.

Теперь оценим количество вершин:
На уровне i количество вершин = 2 ^ (2^k - 2^(k - i))
Количество детей у такой вершины - 2 ^ (2 ^ (k - i - 1))

Тогда на кажом уровне work:
2 ^ (2^k - 2^(k - i)) * (2 ^ (2 ^ (k - i - 1))) ^ 2 =
  = 2 ^ (2^k - 2^(k - i)) * 2 ^ (2 ^ (k - i)) =
  = 2 ^ (2 ^ k)
  = n

Work = n * log(log(n))
Depth = log(log(n))
```

Finally, будем сводить задачу к меньшей за O(1)/O(n),
каждый раз деля массив пополам (просто беря максимум из соседних)
и сделаем так log(log(log(n))) раз.
```
Work = O(n)
Depth = O(log(log(log(n))))
```



## Pipelining

Пусть нам дано 2-3 дерево размера n.
Мы хотим вставить в него m элементов

Если никакие элементы не попадают в одну и ту же дырку, всё ок -
мы сможем обработать такие кейсы leve-by-level
```
Span = O(log(n) * log(m))
```

А вот если попадают, то:
- вставим сначала средний (до конца)
- потом два средних из двух половин
- и тд
```
Span = O(log(1) * log(n) + log(2) * log(n) + ... + log(m) * log(n)) =
  = O(log^2(m) * log(n))
```

А теперь pipelining:
начнем вставлять следующие элементы после того,
как пропихнули следующие на 1 + 1 вверх, чтоб нижний lvl зафиксировался.
```
Span = O(log(m) * (log(n) + log(m)))
```



## Symmetry breaking

Дан цикл, заданный через массив next.

Нужно построить массив color такой, что
любые соседние вершины имеют разный цвет.

Очевидно, что нам нужно минимум три цвета.

1. `color[id] = id`
2. найдем minbit (минимальный индекс), считаем, что за const,
   в котором отличаются `color[id]` и `color[next[id]]`
3. `color[id] = 2 * minbit + color[id] + color[id].get(minbit)`

После одного применения такой штуки,
кол-во цветов превратится из n в 2 * log(n),
пока цветов не станет менее 8.

Применений может быть, очевидно, порядка log*(n)

```
Work = O(log*(n) * n)
Span = O(log*(n) * log(n))
```

Итого у нас есть константное число цветов. - от 0 до C

Теперь закрасим все вершины цвета 0, 1 и 2.
После этого пойдем по следующим цветам
и будем красить вершины в минимальный цвет,
которого нет у соседей.

Поскольку мы красим цвет за цветом,
мы никогда не попытаемся покрасить за одну итерацию
две соседние вершины.

```
Work = O(n)
Span = O(log(n))
```

Однако, если мы совместим обе части алгоритма, work будет `O(log*(n) * n)`.
Хочется меньше.

Проделаем битовую операцию один раз. Получим 2 * log(n) цветов,
каждый из которых <= 2 * log(n)
Теперь хочется их быстро отсортировать.
Идея: сортировка подсчетом за O(n) work и O(log^3(n)) span.

Когда мы все отсортируем, получим те самые списки элементов по цветам
```
Total
Work: O(n)    // honestly O(n * minbit)
Span: O(sorting span) = O(polylog)
```



## List ranking

Дан список из n вершин, заданный массивом next.
Последняя вершина смотрит в себя.

Хотим для каждой вершины найти расстояние до конца.

Можно за O(n * log(n)) work и O(log(n)) span
сделать что-то типа двоичных подъемов, прыгая каждый раз на 2^i.

```
len = [1, 1, 1, ...] // 0 where end
for _ in 0..log(n):
    old_next = next
    old_len = len
    pfor i in 0 until n:
        next = old_next[old_next[i]]
        len = old_len[i] + old_len[old_next[i]]
```

Попробуем уменьшить задачу до n / log(n).

Покрасим вершинки в три цвета.
Вершин какого-то цвета >= n/3.
Выкинем их из списка, попутно запомнив, где они были,
и увеличив соответствующий len до 2-х.

Тогда применив такую штуку log(log(n)) раз
мы сведем задачу к нужному размеру.

```
Work = O(n * log(n))
Span = O(log^2(n) * log(log(n)))
```

### Randomized list ranking:

Будем сводить задачу к меньшей следующим образом:

- рандомно назначим вершинам роли - Tail или Head
- вырежем те вершины, которые сами - Tail и ведут в Head

Теперь докажем, что за O(log(n)) шагов
алгоритм дойдет до размера n / log(n)

Поделим список на последовательные пары - `x[i]`
и будем ждать комбинацию T, H
Тогда вероятность такой комбинации - 1/4 для n/2 пар, =>
в среднем мы будем выкидывать n/8 элементов.

Upper tail theorem:
`P[x > (1 + d) * m] <= e ^ -(m * d^2 / 3), 0 <= d <= 1`

Назовем `x[i]` вероятность того, что пара осталась - т.е. 3/4.

Теперь докажем, что
```
x = x1 + x2 + ...
m = E[x1 + x2 + ..]
P[(x1 + x2 + ...) >= (1 + d) * 3/8 n] <= e ^ (3/8 n * d^2 / 3)
```
Теперь подберем d таким, чтобы (1 + d) * 3/8 n = 7/16 n

d = 1/6

Тогда `P[x >= 7/16 n] <= e ^ -(n / c)`

Значит, вероятность того, что все попытки будут успешными:

```
(1 - e ^ -(n / c)) *
  * (1 - e ^ -(15n/16 / c)) * ... *
  * (1 - e ^ -((n/log(n)) / c)) >=
  (1 - e ^ (n / (c * log(n))) * log 16/15 (log(n)) * >=
  1 - log 16/15 (log(n)) * e ^ (n / (c * log(n)) >=
  1 - 1 / (n^a)
```



## Eulerian circuit

Эйлеров обход - обход дерева по ребрам.
Каждое ребро считается в каком-то смысле двумя ориентированными.
Обход представляет собой цикл вида <u,v1>...<v1,u><u,v2>...<vk,u>

Утверждается, что для каждого ребра мы знаем следующее,
поэтому можем построить массив next[].

Теперь удалим одно из ориентированных ребер,
возвращающих нас в корень. Эйлеров цикл стал путем.

Мы получили массив next[],
который представляет собой связный список ребер.

Теперь запустим на этом list ranking,
получим для каждого ребра число `len[<u, v>]`.

И теперь главное: `len[<u, v>] < len[<v, u>] => parent[v] = u`

В итоге мы научились подвешивать дерево за любую вершину.

### usages

1. Посчитать tin/tout

   Решение: `для <u, v> tin = len[<u, v>], tout = len[<v, u>]`

2. Кол-во вершин в поддереве

3. Глубина вершин



## Rake

Задача: \
Дано бинарное дерево, представляющее собой арифметическое выражение,
в котором листья - числа, а не-листовые узлы - операции +, - и *

Хотим в ситуации, когда у вершины есть хотя бы один лист,
уметь выкидывать эту вершину, "пробрасывая" результат наверх.

Решение: \
Будем хранить в каждой вершине 4 константы: \
`c1 * a * b + c2 * a + c3 * b + c4` \
Где `a` и `b` - еще не посчитанные дети вершины `c`.

Последовательно - понятно, как жить. Осталось научиться жить параллельно.

Упорядочим все листья (что-то про tin).
Будем вырезать только нечетные.
Но пока это не очень помогает - может быть ситуация,
когда конфликт всё равно есть.
```
  x
 / \
1   y
   / \
  2   3
```
Назовем стеблем вершину, у которой есть нечетный лист.
Утверждается, что не бывает трех стеблей, соседних по ребру.
Доказательство перебором, четность не сойдется.

Ещё финт ушами: Если есть два соседних стебля,
то их листы разной направленности. Док-во аналогично.

Итого делим нечетные листья еще на два типа:
нечетные левые и нечетные правые.
Будем сжимать их отдельно, поскольку они друг другу не мешают
(тк. стебли не соседние).

Повторяем это рекурсивно, span - какой-то полилог.



## Rake + Compress

Пусть дана операция (кажется, над группой). Назовем её `+`.
Главное, чтоб был обратный элемент.

А ещё дано дерево с числом детей не более чем const
и некоторым значением в каждой вершине.

Задача: \
Посчитать для каждого поддерева сумму в нем.

Тогда сделаем сначала rake, а потом:
- бросим на каждую вершину, у которой один сын,
  монетку T/H (как в list ranking)
- если у родителя H, а у сына T, вырежем сына (очевидно, запомнив),
  аналогично random list ranking.

Тогда можно поделить вершины на три типа:
T0 - лист
T1 - ровно 1 ребенок
T2 - 2 и более детей

Тогда T0 + T2 -> T2
И T1 -> 15/16 T1




## Lower bounds for max()

Можно ли ускорить бин поиск? \
Нет. \
Можно сделать span = log p (n), work = p * log p (n)

У нас был алгоритм, который ищет max в массиве
в CRCW за log(log(n)) depth и n work.

Докажем, что быстрее нельзя.

На каждом шаге мы можем совершать не более,
чем n сравнений.

По теореме <кого-то-там> после p операций в графе
остается независимое множество размера хотя бы
`n^2 / (2 * m + n)`

Тогда после i сравнений кол-во потенциальных максимумов
будет `C[i] >= n / (3 ^ (2^i - 1))`,
=> требуется хотя бы log(log(n)) операций,
чтобы довести размер множества до 1.




## Connectivity

Хотим быстро искать компоненты связности в графе,
заданном матрицей смежности.

### Первый способ (для плотных графов):

Создадим массив `p[]`, который для каждой вершины указывает
её соседа с наименьшим id.

Итого получили лес деревьев,
подвешенных за вершину с минимальным id,
где из корня есть одно ребро в куда-то.
Такое ребро аккуратно перевесим в саму вершину.

Теперь сделаем следующее:
```
repeat(log(n)) {
    old_p = p
    pfor i in 0 until n:
        p[i] = old_p[old_p[i]]
}
```
Теперь все деревья выглядят как дерево из корня и листьев,
поэтому мы можем быстро их покрасить - сказать, что если parent-a нет,
то мы и есть цвет, а если есть, то наш цвет - цвет parent-а.

Теперь у нас есть `c[]` - цвет каждой вершины, один цвет => одна компонента
(но не наоборот!).
Отсортируем вершины по цвету за O(n * log(n)) work,
далее сожмем каждый цвет в одну вершину и построим новую
матрицу смежности, где ребро между цветами A и B будет тогда и только тогда,
когда было хотя бы одно ребро, связывающее вершины цветов A и B.

Итерация стоит:
```
Work = O(n^2)
Span = O(polylog)
```

А сколько нужно таких итераций?

После каждой итерации становится хотя бы в 2 раза меньше вершин,
поэтому span останется таким же, а work будет все еще O(n^2), потому что
`Work = O(n1^2 + n2^2 / 4 + n3^2 / 16 + ...) = O(n1^2) = O(n^2)`

### Второй способ (для разреженных графов):

Сделаем одну итерацию из предыдущего способа.

Далее:
```
repeate(log(n)) {
    pfor uv in edges {
        // подвесим вершину, если она root,
           но из нее на самом деле есть ребро в другое дерево
        if p[u] == p[p[u]] && p[u] > p[v] {
            p[p[u]] = p[v]
        }
    }

    // куда-то подвешиваем "звезды" (смотрящие сами в себя + их дети), 
       чтобы потом не поломать док-во про уменьшение высот.
       Звезда, которую не подвесить - уже компонента, можно выкинуть.
    pfor uv in edges {
        if v is star && p[u] != p[v] {
            p[p[u]] = p[u]
        }
    }

    // собсна, сжимаем
    pfor v in vertexes {
        p[v] = p[p[v]]
    }

    // тут в 1-м и 2-м циклах есть гонки, 
       мы стоически решили сортировать ребра - "и так сойдет"
}

Work = O(m * log n) // log^2 если сортить
```

Суть в том, что высота деревьев каждый раз уменьшается хотя бы в 3/2 раза,
если подумать - то понятно, почему.




## Biconnectivity

Хотим разбить граф на мосты и блоки.

Сначала через просто connectivity построим остов и подвесим его.

Теперь:
ребро из вершины u к родителю является мостом тогда и только тогда,
когда не существует ребер, ведущих из этого поддерева вовне.

Как это искать?
1. посчитаем для каждой вершины tin и tout.

2. (зная tin и tout знаем размеры поддеревьев)

3. для каждой вершины заведем min_tout и max_tin по всем соседям.
   (отсортируем ребра и сделаем scan на min-max).

4. теперь посчитаем tin и tout для каждого поддерева.

5. если все концы ребер в поддереве v
   попадают в (tin, tout v) то `v->p[v]` - мост.

Итого мы нашли компоненты реберной двусвязности.
Теперь найдем компоненты вершинной двусвязности в этом графе

Отметим, что каждое ребро вне дерева образует базовый цикл.

Для этого ребра и ребер, идущих из концов в предков,
построим двойственные ребра.
А также построим двойственные для ребер, подряд идущих наверх,
если из поддерева вершины между ними есть ребра наружу.




## Strongly connected components

Ищем компоненты сильной связности в связанном графе.

Утверждается, что быстрее, чем за O(m * log(n)) нельзя

1. Берем случайную вершину в графе
2. Обходим все достижимые по прямым ребрам, назовем это множество succ
3. Обходим все достижимые по обратным, назовем это множество pred
4. Компонента сильной связности = succ intersect pred
5. Наблюдение - не бывает компоненты сильной связности,
   которая пересекается с pred или sec частично

Итого у нас есть четыре компоненты - C (собсна, пересечение),
pred, succ и all

Далее мы делаем fork3join от (pred - c), (succ - c) и all - (pred + succ).

Получился +- divide and concurrer алгоритм

Теперь докажем, что work = O(m * log (n)).
Делать это будем, доказывая, что каждое ребро пройдено не более log(n) раз

Сделаем псевдо-top-sort, сначала со сжатыми компонентами,
чтобы нумеровать ациклический граф,
а потом пронумеруем как-то внутри компонент.

Рассмотрим какое-то ребро графа. Пусть оно будет пройдено d(n) раз.
Тогда, исходя из логики упорядочивания:
`d(n) <= 1 + 1/n * sum[i: i..n-1] { d(i) }`

Далее докажем, что `d(r) <= 1 + 1/2 + ... + 1/r`,
через математику вычитания `r * d(r) - (r - 1) * d(r - 1)`

Okay, мы доказали expected.
Теперь хочется доказать, что какую бы мы вершину ни взяли,
время работы будет m * log(n).

Для этого детерминируем алгоритм так:
1. берем случайную вершину
2. начинаем делать dfs по одному ребру в каждую строну
3. когда обойдем до конца p или s, остановимся

Пусть наименьшим множеством оказалось p.

Тогда:
```
Пусть:
c = edges(компонента)
p = edges(pred - c)
s = edges(succ - c)
a = all

T(m) = T(p) + T(m - p - c) + O(p + c)
Далее будет индукция с фокусами на логарифмах 
и использованием того, что m >= 2p
```




## Delta stepping

Хотим найти кратчайшие расстояния от вершины A до всех остальных.

Параллельной дейкстры не бывает.
Зато бывает параллельный форд-белман, но не в этот раз.

Глобальная идея:
- побьем вершины на бакеты B по расстоянию до A:
  0-delta, delta-2delta, ...
- будем релаксировать наименьший бакет, пока тот не кончится

Алгос:
```
fun relax(v: Vertex, x: newDistance) {
    if (x < d[v]) {
        B[d[v] / delta] -= v
        B[x / delta] += v
        d[v] = x
    }
}

for v in V {
    heavy = { edge(vu) > delta }
    light = { edge(vu) <= delta }
}
relax(a, 0)
iteration = 0
while B.isNotEmpty() {
    S = emptySet()
    // пока бакет релаксируется, релаксируем легкие ребра
    while B[iteration].isNotEmpty() {
        // parallel
        rqs = { (u, d[v] + edge(v, u) : v in B[i] && edge(v, u) in light }
        // parallel
        S += B[i]
        B[i] = emptySet()
        pfor (u, x) in rqs {
            relax(u, x)
        }
    }
    // обновляем heavy ребра 
    // parallel
    rqs = { (u, d[v] + edge(v, u) : v in S && edge(v, u) in heavy }
    pfor (u, x) in rqs {
        relax(u, x)
    }
    iteration += 1
}

Где-то перед релаксацией нужен radixSort, 
чтоб не апдейтить одну и ту же вершину дважды на итерации

Fun fact: если ребра натуральные и delta = inf,
получается форд-белман
Если же delta = 1, то дейкстра
```

А теперь математика:

Лемма: \
Пусть P x = число путей в графе, у которых вес меньше x

Утверждается, что число повторных вставок <= P delta \
Повторных релаксаций <= P 2delta

Доказывать будем для случайных графов G(n, d)

Доказательство через мат ожидания числа путей,
меньше либо равных по весу некоторому количеству дельт и
путей, по количеству ребер не превышающих некоторого l.

Итоговый work у нас будет O(n * d) для такого графа.





## BFS

Алгос, как пишут (c cas-ом). \
Non-deterministic, потому что CAS.

```
frontier = [S]
while frontier.isNotEmpty() {
    // parallel
    neighbours = { v.degree for v in frontier }
    sum = parallelScan(neignbours)
    processed = Array(neighbours.size, false)
    target = Array(sum)
    pfor v : frontier {
        pfor u : v.edges() {
            if (processed.cas(u, false, true)) {
                target[//index] = u
            }
        }
    }
}

Work +- O(n)
```

Рандомизированный алгос остовного дерева
с polylog span.

Beta-d декомпозиция графа G

Разобьем граф на части, такие, что
каждая часть связна и ее диаметр не более d,
а число ребер между частями <= Beta * |edges|.

Допустим, мы умеем делать такую декомпозицию. Тогда:

1. Сделаем декомпозицию
2. В каждой компоненте построим дерево
3. Сожмем граф (выкинув ребра-дубли integer sort-ом)
4. Снова декомпозиция

Итого мы уменьшаем число ребер в графе каждый раз
как минимум в Beta раз, поэтому нужно log итераций.

Научимся делать (Beta, O(log(n) / Beta)) декомпозицию
за O(m) work и O(log^3(n)) span.

1. Пошафлим вершины
2. Для k = 10..: выдадим 2^k вершинам число k
3. Соберем frontier = `[S]`
4. Сделаем шаг bfs-а и сделаем frontier += <вершины с числом i>
5. Если вершина пришла из прибавления, она - начало новой компоненты

Поскольку у нас log(n) различных чисел, диаметр компоненты не более log(n)

Почему именно Beta? Потому что статья.
Если коротко, то из-за рандома и экспоненциального распределения
вероятность ребра выжить - Beta.





## Deterministic reservation

Есть некоторое количество алгосов,
которые требуют жадного "прохода слева направо"
и каких-то последовательных действий.
Например, Spanning forest,
Minimum spanning tree, Maximal Independent Set.

Казалось бы, параллелить тут особо нечего, но нет.

Введем такую конкурентную структуру, как Reservation.

```
interface Reservation {
    /**
     * Reservates item if requestor id less than already reserved
     */
    fun reserve(item, id)

    /**
     * Checks if passed item was reserved by id
     */
    fun check(item, id)
}
```

Тогда во всех вышеперечисленных алгоритмах можно брать некоторые d
следующих по порядку перебора элементов и пытаться проделывать
операции для пачки сразу, через reserve() + check()




## String algorithms

### Поиск всех вхождений

Введем массив WITNESS, длинны `min(len / 2, period)`, такой, что
`WITNESS[j] = k < len(WITNESS)`,
где k - какая-то позиция в сдвинутой на j строке такая,
что `original[j + k - 1] != shifted[k]`

А теперь для пары строк Y, Z и пары индексов - i и j
введем следующую операцию:
duel(Y, Z, i, j): Int? -
эта операция возвращает либо i, либо j, либо null \
Эта операция берет строку Z,
прикладывает к ней строку Y со сдвигами i и j,
далее считает `Y_WITNESS[j - i + 1]`
(первый неравный эл-т в сдвинутых строках),
а затем сравнивает эти неравные элементы с соответствующим индексом в Z.
Если кто-то совпал, этого кого-то возвращаем, иначе - null.

Теперь давайте искать шаблон Y в строке Z. Обе строки непериодические.

Поделим строку Z на подстроки размера m/2.

Для каждого блока соберем бинарное дерево дуэлей,
получим число (или null).
Получаем максимум 2n/m чисел и ко всем тупо приложим искомую строку.

```
Work = O(2n / m * m) = O(n)
Span = polylog
```

Теперь про периодические строки: \
Пусть `Y = u^k + v, v = pref(u), Y' = Y[1..2p]`

Теперь сделаем предыдущий алгос для строки u,
найдем позиции, где начинается u,
а прикладывать и проверять будем uuv.

Тогда мы нашли позиции, где может начинаться uuv,
и нам осталось проверить, что в предыдущих (k-2) элементах
с шагом |u| могла начинаться u.


Осталось научиться считать WITNESS.

Пусть мы знаем WITNESS для первых t позиций.
Тогда утверждается, что для любой пары `i, j` такой, что |j - i| < t,
мы знаем WITNESS хотя бы для одного элемента.
Это докажем чуть позже.

Если это так, то мы можем растить префикс по степеням двойки,
каждый раз досчитывая последний неизвестный элемент втупую
(можно, по идее, за log).

Теперь о том, как имея i и j получить WITNESS для кого-то из них:
```
k = WITNESS[j - i + 1]

Первый случай, если k + j - 1 попадает в Y.

Y[k] != Y[k + j - i] =>
(Y[k + j - 1] != Y[k]) || (Y[k + j - 1] != Y[k + j - i]) =>
WITNESS для кого-то мы нашли.

Второй случай:

Y[k] != Y[k + j - i] =>
(Y[k] != Y[k - i + 1]) || (Y[k + j - i] != Y[k - i + 1])
```

Теперь немного пропатчим подсчет WITNESS
и будем считать его префиксами по 2^alpha, не торопясь считать втупую,
при этом учтя, что witness периода мы за просто так не узнаем.
Тогда выйдет намана.

```
Work = O(m)
Span = O(polylog)
```


### Суффиксный массив

`S = [a1, a2, ... an, $, $, $ ...] size of 2^it` \
Хотим получить массив следующего вида: \
`id = [id1, id2, id3, ... idn]` \
Такой, что суффиксы `s[id[x]..n]` упорядочены в порядке x

Алгос:
- посортим суффиксы длинны 1
- посортим суффиксы длинны 2 на основе суффиксов длинны 1
- ан-но со следующими степенями двойки до it

Каждый sort - это bucket sort, поэтому он за O(n),
итоговый алгос - O(n * log(n))


Теперь пропатчим массив id:
```
id[x][q] = id строки, начинающейся с символа номер x с длинной 2^q
Если id[x][q] == id[y][q], то соответствующие строки равны.
```
Как его посчитать?
```
pfor i in 1..n:
    // char as id
    id[i][0] = s[i]
// deduplicate
for q in 1..log(n):

    // reserve, commit
    pfor i in 1..n:
        k1 = id[i][q - 1]
        k2 = id[i + 2^(q-1)][q - 1]
        bb[k1][k2] = i
    pfor i in 1..n:
        id[i][q] = bb[k1][k2]

В текущей реализации BB требует O(n^2) памяти, 
но можно за O(n ^ (1 + eps)) и за O(log^2(n) / eps) span
```


### Суффиксное дерево

Хотим сжатое суффиксное дерево.

Сначала посчитаем все id-шки и все bb массивы из предыдущего пункта.

Для начала построим просто блин дерево
с каждым суффиксом длинны n = 2^k (там ещё где-то $ валяются).

Далее поделим каждый суффикс пополам и смержим одинаковые начала,
каждый раз копируя происходящее в новое дерево.
Тут тоже что-то про reserve, commit.

Вопрос: как искать в таком дереве подстроку p за log?

1. поймем, какой у строки p id по массивам bb, понятно, как
2. пойдем по нашим версионированным деревьям
    - если размер p меньше, чем суффиксы в текущем дереве, идем дальше.
    - если нет, то мы берем id первой "половины" p и ищем вершину
    - если нашли, recursive от этой вершины



### Суффиксный массив за линию

Работает только если символы в строке от 0 до n-1,
потому что radix sort

1. Разобьем строку на последовательные подстроки длинны 3
2. Посортим из radix sort-ом и выдадим id-шки
3. Разделим подстроки по остатку от деления их позиции на 3
4. Остатки 1 и 2 склеим и рекурсивно запустимся
5. Посортим 0 остаток используя 1 и 2
6. Смержим результаты, пользуясь тем, что одно - продолжение другого.

```
Work = O(n)
Span = ?
```





## Sample sort

Сортируем массивчик.

Сначала возьмем sqrt(n) семплов и отсортируем их.

Далее делим массив на sqrt(n) частей и сортим каждую.

Каждую часть побьем на подчасти по семплам.


1. Возьмем sqrt(n) семплов и отсортируем их
2. Делим массив на sqrt(n) частей и сортим каждую.
3. Каждую часть побьем на подчасти по семплам.
4. Переставим подчасти "как нада"
    - сначала соберем матричку блок -> ряд
    - блочно транспонируем матричку
    - зафигачим скан на рядах таблички
    - вставим подчасти
5. Отсортируем теперь внутри новых блоков

`Work = O(n * log^2(n)) // ?`





## Convex hull

Там что-то в духе divide and conquer,
так что опишем процесс поиска общей касательной
для двух оболочек.

За два бин поиска:
1. бин поиск слева
2. бин поиск справа

Это что-то типа log^2(n) span и log^2(n) work

Если заменить бин поиск на корни, то
будет log(n) span и n work.
Уменьшили span и дофига распараллелили.
