# Введение

### Parallel vs concurrent

Parallel system - один синхронный input, 
один синхронный output, параллельность только внутри

Concurrent system - много асинхронных input-ов, 
надо синхронизировать их ручками

## Старые модели

### PRAM - parallel random access machine.

Набор синхронно выполняющихся инструкций, каждое ядрышко по одному такту. 
Инструкции - некоторые простые операции - чтение/запись/сложение и тд.

**Work** - суммарное количество выполненных операций (включая пустые)

**Time** - количество итераций

**Разрешение конфликтов в PRAM**
- EREW - exclusive read, exclusive write
- CREW - concurrent read, exclusive write
- CRCW - concurrent read, concurrent write
  - arbitrary - какое-то из значений будет записано
  - common - все должны записать одно и то же
  - priority - процесс с наибольшим приоритетом выигрывает

Есть теорема, заявляющая, что можно
эмулировать CRCW на EREW за (число итераций) * log(n).

```
Input:
  n - number of processes = 2^k
  id: Process number, 1..n
  A - input array, size n, indices 1..n
  B - output array, same as A


a <- A[id]
B[id] <- a
for h in 1..log(n)
    if id < n / 2^h:
        x <- B[2 * id + 1]
        y <- B[2 * id]
        z <- x + y
        B[id] <- z
    else:
        skip(4)
if id == 1:
    ans <- A[1]


Work: O(n * log(n))
Time: O(log(n))
```


### Parallel for (work-depth model)

Есть parallel for, который выполняется на скольки-то процессах. 
Он же - точка синхронизации. 
Идея - запрятать число процессов куда-то внутрь parallel for-а.
Тело pfor-а выполняется процессорами по одной инструкции.

```
pfor i in 1..n:
    a <- A[id]
    B[id] <- a
for h in i..log(n):
    pfor i in 1..n/2:
        x <- B[2 * id + 1]
        y <- B[2 * id]
        z <- x + y
        B[id] <- z


Work: O(n)        // процессы не простаивают
Depth: O(log(n))  // если число процессов бесконечно
```

### Fork-jon model

Все исполнение программы можно представить в виде
ориентированного ациклического графа (DAG-а). 
Разветвления и слияния в таком графе - это fork() или join(), 
они же - точки синхронизации. 
Каждую вершину такого графа мы считаем за O(1), 
однако для удобства можно обозначить бамбук в графе за одну не-O(1)-вершину.
У такого исполнения есть work и span.

**Work** - количество вершин в DAG-е

**Span** - время выполнения при бесконечном числе процессов, 
оно же - самый длинный путь в DAG-е

pfor на fork-join модели требует O(n) work и O(log(n)) span. 

Из приятного:

```
pfor i = 1..n:
    pfor j = 1..m:
        ++x[i][j]

Work: O(n * m)
Span: O(log(n) + log(m))
```

Из не очень приятного - Scheduling

**Теорема Брента:**

Возьмем level-by-level scheduler. 
Тогда время T его работы будет `T <= W / P + S * (P - 1) / P`

Док-во: Назовем W[i] кол-во вершин на i-м шаге. Тогда
```
T = Sum(i = 1..S) { ceil(W[i]/P) } <= 
    <= Sum(..) { W[i]/P + 1 - 1/P } =
    = W/P + S * (P - 1)/P
```

**Ещё теорема: любой Greedy Scheduler подходит под то же условие**

Work token - платим 1, если сделали что-то полезное

Idle token - платим 1, если отдыхаем

Work token-ов, очевидно, W. Сколько idle токенов?

Утверждение - каждый раз, когда мы платим dile token,
оставшийся span становится на 1 меньше. 
Кажется, можно доказать по индукции - для одного запуска это правда, 
для остальных тогда +- очевидно.

Тогда idle токенов может быть не более, чем S * (P - 1)

Тогда суммарно токенов - W + S * (P - 1)

Суммарное время `T <= W / P + S * (P - 1) / P

Когда делаем алгоритм:
1. Ищем work-optimal алгоритм
2. После этого уменьшаем span

В рамках курса пытаемся сделать Work - полином, Span - полилог, 
но ирл, поскольку процессов мало, достаточно условного sqrt(n) Span.
